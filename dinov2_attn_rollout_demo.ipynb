{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# DINOv2 Attention Rollout Demo\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºå¦‚ä½•ï¼š\n",
    "1) åŠ è½½é¢„è®­ç»ƒçš„ DINOv2 ViT å¹¶æå–å›¾åƒç‰¹å¾ï¼›\n",
    "2) ä½¿ç”¨ **Attention Rollout** å¯è§†åŒ–æ³¨æ„åŠ›å›¾ã€‚\n",
    "\n",
    "> æ¨¡å‹ï¼šHugging Face `facebook/dinov2-small`ï¼ˆæ”¯æŒ CPU/GPUï¼‰\n",
    "\n",
    "ğŸ‘‰ ä½ å¯ä»¥ï¼š\n",
    "- ç›´æ¥è·‘æˆ‘ä»¬æä¾›çš„ç¤ºä¾‹å›¾ç‰‡ï¼›\n",
    "- æˆ–è€…ä¸Šä¼ ä½ è‡ªå·±çš„æœºå™¨äººé‡‡é›†å›¾åƒè¿›è¡Œå¯è§†åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade torch torchvision transformers pillow matplotlib opencv-python\n",
    "import torch, torchvision\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests, io, cv2\n",
    "from typing import List\n",
    "print('Torch:', torch.__version__, '| CUDA available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = 'facebook/dinov2-small'  # çº¦22Må‚æ•°ï¼Œè¯¾å ‚æ¼”ç¤ºè¶³å¤Ÿ\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "model = Dinov2Model.from_pretrained(MODEL_ID, output_attentions=True).to(device).eval()\n",
    "print('Loaded', MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utils"
   },
   "outputs": [],
   "source": [
    "def load_image_from_url(url: str, size: int = 518) -> Image.Image:\n",
    "    # DINOv2 å»ºè®®è¾“å…¥å°ºå¯¸èƒ½è¢« patch_size æ•´é™¤ï¼ˆsmall æ¨¡å‹ patch=14ï¼‰ï¼Œ518 åœ¨ç¤ºä¾‹ä¸­å¸¸ç”¨\n",
    "    img = Image.open(io.BytesIO(requests.get(url, timeout=10).content)).convert('RGB')\n",
    "    img = img.resize((size, size))\n",
    "    return img\n",
    "\n",
    "def preprocess(img: Image.Image):\n",
    "    inputs = processor(images=img, return_tensors='pt')\n",
    "    return {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "def attention_rollout(attentions: List[torch.Tensor], head_fusion: str = 'mean', start_layer: int = 0):\n",
    "    \"\"\"\n",
    "    attentions: list of (B, num_heads, T, T)\n",
    "    è¿”å› (B, T) CLS->token çš„æ³¨æ„åŠ›å¾—åˆ†ï¼ˆå·²rolloutï¼‰\n",
    "    å‚è€ƒï¼šAbnar & Zuidema, 2020ï¼›ViT è®ºæ–‡é™„å½•åšæ³•\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # é€‰å±‚èŒƒå›´\n",
    "        attns = attentions[start_layer:]\n",
    "        # èåˆ headï¼ˆmean/max/minï¼‰\n",
    "        if head_fusion == 'mean':\n",
    "            attns = [a.mean(dim=1) for a in attns]  # (B, T, T)\n",
    "        elif head_fusion == 'max':\n",
    "            attns = [a.max(dim=1).values for a in attns]\n",
    "        elif head_fusion == 'min':\n",
    "            attns = [a.min(dim=1).values for a in attns]\n",
    "        else:\n",
    "            raise ValueError('head_fusion must be one of [mean, max, min]')\n",
    "\n",
    "        # åŠ  I å¹¶é‡æ–°å½’ä¸€åŒ–ï¼ˆè€ƒè™‘æ®‹å·®å¯¹ä¿¡æ¯æµçš„å½±å“ï¼‰\n",
    "        attns = [a + torch.eye(a.size(-1), device=a.device).unsqueeze(0) for a in attns]\n",
    "        attns = [a / a.sum(dim=-1, keepdim=True) for a in attns]\n",
    "\n",
    "        # é€å±‚ç›¸ä¹˜\n",
    "        rollout = attns[0]\n",
    "        for a in attns[1:]:\n",
    "            rollout = torch.bmm(a, rollout)\n",
    "\n",
    "        # å– CLS token å¯¹æ‰€æœ‰ token çš„æ³¨æ„åŠ›ï¼ˆCLS åœ¨ç´¢å¼•0ï¼‰\n",
    "        cls_attn = rollout[:, 0]  # (B, T)\n",
    "        return cls_attn\n",
    "\n",
    "def show_attention_on_image(img: Image.Image, attn_map_2d: np.ndarray, alpha: float = 0.5):\n",
    "    # å°†æ³¨æ„åŠ›å›¾ resize åˆ°åŸå›¾å¤§å°ï¼Œå¹¶å åŠ å¯è§†åŒ–\n",
    "    h, w = img.size[1], img.size[0]\n",
    "    attn = cv2.resize(attn_map_2d, (w, h))\n",
    "    attn = (attn - attn.min()) / (attn.max() - attn.min() + 1e-8)\n",
    "    heat = (plt.cm.jet(attn)[..., :3] * 255).astype(np.uint8)\n",
    "    over = cv2.addWeighted(np.array(img), 1 - alpha, heat, alpha, 0)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(over)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_images"
   },
   "outputs": [],
   "source": [
    "# ç¤ºä¾‹å›¾ç‰‡ï¼ˆä½ å¯æ›¿æ¢ä¸ºæœºå™¨äººæ‹æ‘„å›¾åƒçš„ URLï¼‰\n",
    "IMAGE_URLS = [\n",
    "  'content/cat.jpg',  # cat\n",
    "  'content/dog.jpg',    # dog\n",
    "  'content/car.jpg',    # car\n",
    "  'content/plane.jpg'     # airplane\n",
    "]\n",
    "print('Loaded', len(IMAGE_URLS), 'sample image URLs.')\n",
    "\n",
    "HEAD_FUSION = 'mean'   # å¯é€‰: 'mean' | 'max' | 'min'\n",
    "START_LAYER = 0        # ä»ç¬¬å‡ å±‚å¼€å§‹åš rolloutï¼ˆå¯è°ƒä»¥è§‚å¯Ÿæµ…/æ·±å±‚å·®å¼‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_demo"
   },
   "outputs": [],
   "source": [
    "for i, url in enumerate(IMAGE_URLS):\n",
    "    print(f'\\n=== Image {i+1}: {url} ===')\n",
    "    img = load_image_from_url(url)\n",
    "    inputs = preprocess(img)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # outputs: last_hidden_state, pooler_output, attentions\n",
    "\n",
    "    # 1) å…¨å±€ç‰¹å¾ï¼šCLS æˆ–å¹³å‡æ± åŒ–\n",
    "    cls_feat = outputs.last_hidden_state[:, 0]              # (B, D)\n",
    "    mean_feat = outputs.last_hidden_state.mean(dim=1)       # (B, D)\n",
    "    print('CLS feature shape:', tuple(cls_feat.shape), '| mean-pooled shape:', tuple(mean_feat.shape))\n",
    "\n",
    "    # 2) Attention Rollout\n",
    "    # outputs.attentions: list[L] of (B, heads, T, T)\n",
    "    attentions = [a.detach() for a in outputs.attentions]\n",
    "    cls_to_tokens = attention_rollout(attentions, head_fusion=HEAD_FUSION, start_layer=START_LAYER)  # (B, T)\n",
    "\n",
    "    # å»é™¤ CLS è‡ªèº«ï¼Œå‰©ä¸‹ patch tokenï¼ˆDINOv2-small é»˜è®¤ patch=14ï¼Œè¾“å…¥518x518 => 37x37=1369 ä¸ªpatch + 1 cls => T=1370ï¼‰\n",
    "    b, t = cls_to_tokens.shape\n",
    "    patch_scores = cls_to_tokens[:, 1:]  # (B, 37*37)\n",
    "    grid = int(np.sqrt(patch_scores.shape[1]))\n",
    "    attn_map = patch_scores.reshape(b, grid, grid).cpu().numpy()[0]\n",
    "    show_attention_on_image(img, attn_map)\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_cell"
   },
   "outputs": [],
   "source": [
    "# å¯é€‰ï¼šä¸Šä¼ ä½ è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œå¯è§†åŒ–ï¼ˆè¯¾å ‚æ—¶å¯ç”¨ï¼‰\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # é€‰æ‹©ä¸€å¼ æˆ–å¤šå¼ å›¾ç‰‡\n",
    "for fname in uploaded:\n",
    "    img = Image.open(io.BytesIO(uploaded[fname])).convert('RGB').resize((518, 518))\n",
    "    inputs = preprocess(img)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    attentions = [a.detach() for a in outputs.attentions]\n",
    "    cls_to_tokens = attention_rollout(attentions, head_fusion='mean', start_layer=0)\n",
    "    patch_scores = cls_to_tokens[:, 1:]\n",
    "    grid = int(np.sqrt(patch_scores.shape[1]))\n",
    "    attn_map = patch_scores.reshape(1, grid, grid).cpu().numpy()[0]\n",
    "    print('Visualizing:', fname)\n",
    "    show_attention_on_image(img, attn_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## è®²è§£è¦ç‚¹é€Ÿè®°\n",
    "- **Feature Extraction**ï¼š`last_hidden_state` çš„ `CLS` å‘é‡å¯ä½œå…¨å±€è¡¨å¾ï¼Œæˆ–å¯¹ token å¹³å‡å¾—åˆ° `mean-pooled` ç‰¹å¾ã€‚\n",
    "- **Attention Rollout**ï¼šå°†æ¯å±‚æ³¨æ„åŠ›åš head èåˆï¼ˆmean/max/minï¼‰ï¼ŒåŠ ä¸Šå•ä½çŸ©é˜µåå½’ä¸€åŒ–å¹¶é€å±‚ç›¸ä¹˜ï¼Œå¾—åˆ° CLSâ†’å„ token çš„â€œä¿¡æ¯æµâ€å¼ºåº¦ã€‚\n",
    "- **è§‚å¯Ÿ**ï¼šæµ…å±‚æ›´åƒè¾¹ç¼˜/çº¹ç†ï¼Œæ·±å±‚æ›´èšç„¦ç‰©ä½“ä¸ä¸Šä¸‹æ–‡è¯­ä¹‰åŒºåŸŸã€‚è¯¾å ‚å¯åˆ‡æ¢ `START_LAYER`/`HEAD_FUSION` å±•ç°å·®å¼‚ã€‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
