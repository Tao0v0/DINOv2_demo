{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¦• DINOv2 ç‰¹å¾å¯è§†åŒ–æ¼”ç¤º\n",
    "# DINOv2 Feature Visualization Demo\n",
    "\n",
    "è¿™ä¸ªnotebookå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨DINOv2æ¨¡å‹è¿›è¡Œç‰¹å¾å¯è§†åŒ–ï¼Œç±»ä¼¼äºåŸè®ºæ–‡ä¸­çš„æ•ˆæœã€‚\n",
    "\n",
    "This notebook demonstrates how to use DINOv2 models for feature visualization, similar to the effects shown in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install opencv-python-headless\n",
    "!pip install matplotlib\n",
    "!pip install Pillow\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ–¥ï¸  CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dinov2_model(model_name='dinov2_vits14'):\n",
    "    \"\"\"\n",
    "    åŠ è½½DINOv2æ¨¡å‹\n",
    "    Load DINOv2 model\n",
    "    model_name: 'dinov2_vits14', 'dinov2_vitb14', 'dinov2_vitl14', 'dinov2_vitg14'\n",
    "    \"\"\"\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    model = torch.hub.load('facebookresearch/dinov2', model_name, pretrained=True)\n",
    "    model.eval()\n",
    "    print(f\"âœ… {model_name} loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path_or_url, size=(224, 224)):\n",
    "    \"\"\"\n",
    "    é¢„å¤„ç†å›¾ç‰‡\n",
    "    Preprocess image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url)\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "        else:\n",
    "            image = Image.open(image_path_or_url)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºRGBæ ¼å¼\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # è°ƒæ•´å¤§å°\n",
    "        image = image.resize(size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–\n",
    "        image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        # ImageNetæ ‡å‡†åŒ–\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        image_tensor = (image_tensor - mean) / std\n",
    "        \n",
    "        return image_tensor.unsqueeze(0), image\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, image_tensor):\n",
    "    \"\"\"\n",
    "    æå–ç‰¹å¾\n",
    "    Extract features\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # è·å–patchç‰¹å¾ (ä¸åŒ…å«CLS token)\n",
    "        features = model.forward_features(image_tensor)\n",
    "        # ç§»é™¤CLS tokenï¼Œåªä¿ç•™patch tokens\n",
    "        if isinstance(features, dict):\n",
    "            patch_features = features['x_norm_patchtokens']  # Shape: [1, num_patches, feature_dim]\n",
    "        else:\n",
    "            # å¦‚æœè¿”å›çš„ä¸æ˜¯å­—å…¸ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨\n",
    "            patch_features = features[:, 1:]  # ç§»é™¤CLS token\n",
    "        \n",
    "    return patch_features\n",
    "\n",
    "def visualize_attention_map(features, original_image, patch_size=14, image_size=224):\n",
    "    \"\"\"\n",
    "    å°†ç‰¹å¾è½¬æ¢ä¸ºæ³¨æ„åŠ›çƒ­åŠ›å›¾\n",
    "    Convert features to attention heatmap\n",
    "    \"\"\"\n",
    "    # è®¡ç®—patchçš„æ•°é‡\n",
    "    num_patches = image_size // patch_size\n",
    "    \n",
    "    # å¯¹ç‰¹å¾è¿›è¡Œå¤„ç†\n",
    "    features_2d = features.squeeze(0)  # [num_patches*num_patches, feature_dim]\n",
    "    \n",
    "    # ç®€å•çš„ç‰¹å¾èšåˆï¼šå–ç‰¹å¾çš„å‡å€¼ä½œä¸º\"æ³¨æ„åŠ›\"åˆ†æ•°\n",
    "    attention_scores = torch.mean(torch.abs(features_2d), dim=1)  # [num_patches*num_patches]\n",
    "    \n",
    "    # é‡å¡‘ä¸º2Dç½‘æ ¼\n",
    "    attention_map = attention_scores.view(num_patches, num_patches)\n",
    "    \n",
    "    # ä¸Šé‡‡æ ·åˆ°åŸå›¾å¤§å°\n",
    "    attention_map = attention_map.unsqueeze(0).unsqueeze(0)  # [1, 1, 16, 16]\n",
    "    attention_map = F.interpolate(\n",
    "        attention_map, \n",
    "        size=(image_size, image_size), \n",
    "        mode='bilinear', \n",
    "        align_corners=False\n",
    "    )\n",
    "    attention_map = attention_map.squeeze().numpy()\n",
    "    \n",
    "    # å½’ä¸€åŒ–åˆ°[0,1]\n",
    "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min() + 1e-8)\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "def create_colored_attention_map(attention_map):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºå½©è‰²æ³¨æ„åŠ›å›¾\n",
    "    Create colored attention map\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨é¢œè‰²æ˜ å°„åˆ›å»ºå½©è‰²çƒ­åŠ›å›¾\n",
    "    colored_map = cv2.applyColorMap((attention_map * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    colored_map = cv2.cvtColor(colored_map, cv2.COLOR_BGR2RGB)\n",
    "    return colored_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_dinov2():\n",
    "    \"\"\"\n",
    "    ä¸»æ¼”ç¤ºå‡½æ•°\n",
    "    Main demo function\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¦• DINOv2 ç‰¹å¾å¯è§†åŒ–æ¼”ç¤º\")\n",
    "    print(\"ğŸ¦• DINOv2 Feature Visualization Demo\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # åŠ è½½ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹ç‰ˆæœ¬\n",
    "    print(\"ğŸ“¥ Loading DINOv2 models...\")\n",
    "    model_v1 = load_dinov2_model('dinov2_vits14')  # ä½œä¸ºDINOå±•ç¤º\n",
    "    model_v2 = load_dinov2_model('dinov2_vitb14')  # ä½œä¸ºDINOv2å±•ç¤º\n",
    "    \n",
    "    # ä½¿ç”¨ç¤ºä¾‹å›¾ç‰‡URLï¼ˆå°ç‹—å›¾ç‰‡ï¼‰\n",
    "    # ä½ å¯ä»¥æ›¿æ¢ä¸ºä½ è‡ªå·±çš„å›¾ç‰‡URLæˆ–æœ¬åœ°è·¯å¾„\n",
    "    image_url = \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400&h=300&fit=crop\"\n",
    "    \n",
    "    print(\"ğŸ–¼ï¸  Processing image...\")\n",
    "    image_tensor, original_image = preprocess_image(image_url)\n",
    "    \n",
    "    if image_tensor is None:\n",
    "        print(\"âŒ Failed to process image. Please check the URL or path.\")\n",
    "        return\n",
    "    \n",
    "    # æå–ç‰¹å¾\n",
    "    print(\"ğŸ” Extracting features from DINO model...\")\n",
    "    features_v1 = extract_features(model_v1, image_tensor)\n",
    "    \n",
    "    print(\"ğŸ” Extracting features from DINOv2 model...\")\n",
    "    features_v2 = extract_features(model_v2, image_tensor)\n",
    "    \n",
    "    # åˆ›å»ºæ³¨æ„åŠ›å›¾\n",
    "    print(\"ğŸ¨ Creating attention maps...\")\n",
    "    attention_map_v1 = visualize_attention_map(features_v1, original_image)\n",
    "    attention_map_v2 = visualize_attention_map(features_v2, original_image)\n",
    "    \n",
    "    # åˆ›å»ºå½©è‰²çƒ­åŠ›å›¾\n",
    "    colored_map_v1 = create_colored_attention_map(attention_map_v1)\n",
    "    colored_map_v2 = create_colored_attention_map(attention_map_v2)\n",
    "    \n",
    "    # å¯è§†åŒ–ç»“æœ\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # åŸå›¾\n",
    "    axes[0, 0].imshow(original_image)\n",
    "    axes[0, 0].set_title(\"Original Image\", fontsize=14, fontweight='bold', pad=20)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # DINOç‰¹å¾å›¾\n",
    "    axes[0, 1].imshow(colored_map_v1)\n",
    "    axes[0, 1].set_title(\"DINO\", fontsize=14, fontweight='bold', pad=20, \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'))\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # DINOv2ç‰¹å¾å›¾\n",
    "    axes[1, 1].imshow(colored_map_v2)\n",
    "    axes[1, 1].set_title(\"DINOv2\", fontsize=14, fontweight='bold', pad=20,\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen'))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # éšè—å·¦ä¸‹è§’å­å›¾\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"ğŸ¦• DINOv2 Feature Visualization Demo\", fontsize=16, fontweight='bold', y=0.95)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Demo completed!\")\n",
    "    print(\"\\nğŸ“ è¯´æ˜ / Explanation:\")\n",
    "    print(\"- åŸå›¾æ˜¾ç¤ºäº†è¾“å…¥çš„å›¾åƒ / Original image shows the input\")\n",
    "    print(\"- DINOå’ŒDINOv2æ˜¾ç¤ºäº†ä¸åŒç‰ˆæœ¬æ¨¡å‹æå–çš„ç‰¹å¾çƒ­åŠ›å›¾ / DINO and DINOv2 show feature heatmaps from different model versions\")\n",
    "    print(\"- é¢œè‰²è¶Šäº®çš„åŒºåŸŸè¡¨ç¤ºæ¨¡å‹è®¤ä¸ºè¶Šé‡è¦çš„ç‰¹å¾ / Brighter colors indicate more important features\")\n",
    "    print(\"- DINOv2ç›¸æ¯”DINOé€šå¸¸èƒ½æ•è·æ›´ä¸°å¯Œå’Œå‡†ç¡®çš„è¯­ä¹‰ä¿¡æ¯ / DINOv2 typically captures richer and more accurate semantic information than DINO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ è¿è¡Œä¸»æ¼”ç¤º\n",
    "# Run the main demo\n",
    "demo_dinov2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom_image_section",
   "metadata": {},
   "source": [
    "## ğŸ¯ è‡ªå®šä¹‰å›¾ç‰‡å¤„ç†\n",
    "## Custom Image Processing\n",
    "\n",
    "ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°æ¥å¤„ç†ä½ è‡ªå·±çš„å›¾ç‰‡ï¼š\n",
    "\n",
    "You can use the function below to process your own images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_custom_image(image_path):\n",
    "    \"\"\"\n",
    "    å¤„ç†è‡ªå®šä¹‰å›¾ç‰‡çš„å‡½æ•°\n",
    "    Function to process custom images\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ–¼ï¸  Processing custom image: {image_path}\")\n",
    "    \n",
    "    model_v1 = load_dinov2_model('dinov2_vits14')\n",
    "    model_v2 = load_dinov2_model('dinov2_vitb14')\n",
    "    \n",
    "    image_tensor, original_image = preprocess_image(image_path)\n",
    "    \n",
    "    if image_tensor is None:\n",
    "        print(\"âŒ Failed to process image. Please check the URL or path.\")\n",
    "        return\n",
    "    \n",
    "    features_v1 = extract_features(model_v1, image_tensor)\n",
    "    features_v2 = extract_features(model_v2, image_tensor)\n",
    "    \n",
    "    attention_map_v1 = visualize_attention_map(features_v1, original_image)\n",
    "    attention_map_v2 = visualize_attention_map(features_v2, original_image)\n",
    "    \n",
    "    colored_map_v1 = create_colored_attention_map(attention_map_v1)\n",
    "    colored_map_v2 = create_colored_attention_map(attention_map_v2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(colored_map_v1)\n",
    "    axes[1].set_title(\"DINO Features\", fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(colored_map_v2)\n",
    "    axes[2].set_title(\"DINOv2 Features\", fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Custom image processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ ä½¿ç”¨ç¤ºä¾‹ / Usage Examples:\n",
    "# \n",
    "# å¤„ç†ç½‘ç»œå›¾ç‰‡ / Process online image:\n",
    "# process_custom_image(\"https://your-image-url.com/image.jpg\")\n",
    "# \n",
    "# å¤„ç†æœ¬åœ°å›¾ç‰‡ / Process local image:\n",
    "# process_custom_image(\"/path/to/your/image.jpg\")\n",
    "\n",
    "# ç¤ºä¾‹ï¼šå¤„ç†å¦ä¸€å¼ ç‹—çš„å›¾ç‰‡\n",
    "# Example: Process another dog image\n",
    "example_url = \"https://images.unsplash.com/photo-1534361960057-19889db9621e?w=400&h=300&fit=crop\"\n",
    "process_custom_image(example_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## ğŸ“ æ€»ç»“ / Summary\n",
    "\n",
    "è¿™ä¸ªdemoå±•ç¤ºäº†ï¼š\n",
    "This demo demonstrates:\n",
    "\n",
    "1. **æ¨¡å‹åŠ è½½** / **Model Loading**: å¦‚ä½•åŠ è½½é¢„è®­ç»ƒçš„DINOv2æ¨¡å‹\n",
    "2. **ç‰¹å¾æå–** / **Feature Extraction**: å¦‚ä½•ä»å›¾åƒä¸­æå–æ·±åº¦ç‰¹å¾\n",
    "3. **å¯è§†åŒ–** / **Visualization**: å¦‚ä½•å°†ç‰¹å¾è½¬æ¢ä¸ºç›´è§‚çš„çƒ­åŠ›å›¾\n",
    "4. **å¯¹æ¯”åˆ†æ** / **Comparative Analysis**: ä¸åŒæ¨¡å‹ç‰ˆæœ¬çš„ç‰¹å¾å·®å¼‚\n",
    "\n",
    "ğŸ”¬ **æ•™å­¦è¦ç‚¹** / **Teaching Points**:\n",
    "- DINOv2æ˜¯è‡ªç›‘ç£å­¦ä¹ çš„é‡è¦çªç ´\n",
    "- ç‰¹å¾å¯è§†åŒ–å¸®åŠ©ç†è§£æ¨¡å‹\"çœ‹åˆ°\"ä»€ä¹ˆ\n",
    "- ä¸åŒæ¨¡å‹æ¶æ„äº§ç”Ÿä¸åŒçš„ç‰¹å¾è¡¨ç¤º\n",
    "\n",
    "ğŸ“š **è¿›ä¸€æ­¥å­¦ä¹ ** / **Further Learning**:\n",
    "- [DINOv2 Paper](https://arxiv.org/abs/2304.07193)\n",
    "- [Facebook Research DINOv2](https://github.com/facebookresearch/dinov2)\n",
    "- [Vision TransformeråŸç†](https://arxiv.org/abs/2010.11929)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}