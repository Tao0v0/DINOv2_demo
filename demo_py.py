import torch
import gradio as gr
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import os
from transformers import AutoImageProcessor, AutoModel
from utils import compute_similarity, visualize_features, load_sample_images
import warnings
warnings.filterwarnings("ignore")

class DINOv2Demo:
    def __init__(self):
        print("ğŸš€ åŠ è½½DINOv2æ¨¡å‹ä¸­...")
        self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')
        self.model = AutoModel.from_pretrained('facebook/dinov2-base')
        self.model.eval()
        
        # åŠ è½½ç¤ºä¾‹å›¾ç‰‡
        self.sample_images, self.sample_features = load_sample_images(self.processor, self.model)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def extract_features(self, image):
        """æå–å›¾åƒç‰¹å¾"""
        if image is None:
            return None
        
        # é¢„å¤„ç†å›¾åƒ
        inputs = self.processor(images=image, return_tensors="pt")
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨CLS tokenä½œä¸ºå…¨å±€ç‰¹å¾
            features = outputs.last_hidden_state[:, 0, :].numpy()
        
        return features
    
    def find_similar_images(self, query_image, top_k=5):
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å›¾åƒ"""
        if query_image is None:
            return None, "è¯·ä¸Šä¼ æŸ¥è¯¢å›¾åƒ"
        
        # æå–æŸ¥è¯¢å›¾åƒç‰¹å¾
        query_features = self.extract_features(query_image)
        if query_features is None:
            return None, "ç‰¹å¾æå–å¤±è´¥"
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, sample_feature in enumerate(self.sample_features):
            similarity = compute_similarity(query_features, sample_feature)
            similarities.append((similarity, i))
        
        # æ’åºå¹¶è·å–top-k
        similarities.sort(reverse=True)
        
        # åˆ›å»ºç»“æœå›¾åƒ
        fig, axes = plt.subplots(1, min(top_k, len(similarities)), figsize=(15, 3))
        if top_k == 1:
            axes = [axes]
        
        result_text = f"æ‰¾åˆ° {min(top_k, len(similarities))} å¼ æœ€ç›¸ä¼¼çš„å›¾ç‰‡:\n\n"
        
        for idx, (similarity, img_idx) in enumerate(similarities[:top_k]):
            if idx < len(axes):
                axes[idx].imshow(self.sample_images[img_idx])
                axes[idx].set_title(f'ç›¸ä¼¼åº¦: {similarity:.3f}')
                axes[idx].axis('off')
                result_text += f"{idx+1}. å›¾ç‰‡ {img_idx+1}: ç›¸ä¼¼åº¦ {similarity:.3f}\n"
        
        plt.tight_layout()
        plt.savefig('similarity_results.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        return 'similarity_results.png', result_text
    
    def visualize_feature_space(self, method="PCA"):
        """å¯è§†åŒ–ç‰¹å¾ç©ºé—´"""
        if len(self.sample_features) < 2:
            return None, "éœ€è¦è‡³å°‘2å¼ å›¾ç‰‡æ‰èƒ½å¯è§†åŒ–"
        
        features_array = np.vstack(self.sample_features)
        
        if method == "PCA":
            reducer = PCA(n_components=2)
            reduced_features = reducer.fit_transform(features_array)
            title = "DINOv2ç‰¹å¾çš„PCAå¯è§†åŒ–"
            explained_var = reducer.explained_variance_ratio_
            subtitle = f"è§£é‡Šæ–¹å·®æ¯”: PC1={explained_var[0]:.2f}, PC2={explained_var[1]:.2f}"
        else:  # t-SNE
            reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features_array)-1))
            reduced_features = reducer.fit_transform(features_array)
            title = "DINOv2ç‰¹å¾çš„t-SNEå¯è§†åŒ–"
            subtitle = "å±•ç¤ºé«˜ç»´ç‰¹å¾çš„éçº¿æ€§ç»“æ„"
        
        # åˆ›å»ºå¯è§†åŒ–
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], 
                            c=range(len(reduced_features)), cmap='tab10', s=100, alpha=0.7)
        
        # æ·»åŠ å›¾ç‰‡ç¼–å·æ ‡ç­¾
        for i, (x, y) in enumerate(reduced_features):
            plt.annotate(f'å›¾{i+1}', (x, y), xytext=(5, 5), textcoords='offset points',
                        fontsize=10, fontweight='bold')
        
        plt.title(title, fontsize=16, fontweight='bold')
        plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ†' if method == "PCA" else 'ç»´åº¦ 1', fontsize=12)
        plt.ylabel('ç¬¬äºŒä¸»æˆåˆ†' if method == "PCA" else 'ç»´åº¦ 2', fontsize=12)
        plt.figtext(0.5, 0.02, subtitle, ha='center', fontsize=10, style='italic')
        
        plt.grid(True, alpha=0.3)
        plt.colorbar(scatter, label='å›¾ç‰‡ç´¢å¼•')
        plt.tight_layout()
        plt.savefig('feature_visualization.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        explanation = f"""
        ğŸ“Š ç‰¹å¾ç©ºé—´å¯è§†åŒ–è¯´æ˜:
        
        â€¢ æ¯ä¸ªç‚¹ä»£è¡¨ä¸€å¼ å›¾ç‰‡åœ¨DINOv2ç‰¹å¾ç©ºé—´ä¸­çš„ä½ç½®
        â€¢ è·ç¦»è¶Šè¿‘çš„ç‚¹è¡¨ç¤ºå›¾ç‰‡åœ¨è¯­ä¹‰ä¸Šè¶Šç›¸ä¼¼
        â€¢ {method}å°†768ç»´ç‰¹å¾é™åˆ°2ç»´ä¾¿äºè§‚å¯Ÿ
        â€¢ é¢œè‰²å’Œç¼–å·å¸®åŠ©è¯†åˆ«ä¸åŒå›¾ç‰‡
        
        ğŸ” è§‚å¯Ÿè¦ç‚¹:
        â€¢ ç›¸ä¼¼å†…å®¹çš„å›¾ç‰‡æ˜¯å¦èšé›†åœ¨ä¸€èµ·?
        â€¢ ä¸åŒç±»åˆ«çš„å›¾ç‰‡æ˜¯å¦åˆ†å¸ƒåœ¨ä¸åŒåŒºåŸŸ?
        â€¢ è¿™åæ˜ äº†DINOv2å­¦åˆ°äº†ä»€ä¹ˆæ ·çš„è¡¨ç¤º?
        """
        
        return 'feature_visualization.png', explanation

def create_demo():
    demo_instance = DINOv2Demo()
    
    with gr.Blocks(title="DINOv2 è¯¾å ‚æ¼”ç¤º", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¦• DINOv2 è‡ªç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ æ¼”ç¤º
        
        **DINOv2** æ˜¯Metaå¼€å‘çš„è‡ªç›‘ç£è§†è§‰Transformerï¼Œæ— éœ€æ ‡æ³¨æ•°æ®å°±èƒ½å­¦ä¹ å¼ºå¤§çš„å›¾åƒè¡¨ç¤ºã€‚
        
        ## ğŸ¯ æ ¸å¿ƒèƒ½åŠ›å±•ç¤º
        """)
        
        with gr.Tabs():
            # Tab 1: å›¾åƒç›¸ä¼¼æ€§æœç´¢
            with gr.TabItem("ğŸ” å›¾åƒç›¸ä¼¼æ€§æœç´¢"):
                gr.Markdown("### ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„å›¾ç‰‡")
                
                with gr.Row():
                    with gr.Column():
                        query_input = gr.Image(type="pil", label="ä¸Šä¼ æŸ¥è¯¢å›¾ç‰‡")
                        similarity_btn = gr.Button("ğŸ” æŸ¥æ‰¾ç›¸ä¼¼å›¾ç‰‡", variant="primary")
                        top_k_slider = gr.Slider(1, 8, 5, label="æ˜¾ç¤ºå‰Kä¸ªç»“æœ")
                    
                    with gr.Column():
                        similarity_output = gr.Image(label="ç›¸ä¼¼å›¾ç‰‡ç»“æœ")
                        similarity_text = gr.Textbox(label="è¯¦ç»†ç»“æœ", lines=6)
                
                similarity_btn.click(
                    demo_instance.find_similar_images,
                    inputs=[query_input, top_k_slider],
                    outputs=[similarity_output, similarity_text]
                )
            
            # Tab 2: ç‰¹å¾ç©ºé—´å¯è§†åŒ–
            with gr.TabItem("ğŸ“Š ç‰¹å¾ç©ºé—´å¯è§†åŒ–"):
                gr.Markdown("### æŸ¥çœ‹DINOv2å¦‚ä½•åœ¨é«˜ç»´ç©ºé—´ä¸­ç»„ç»‡å›¾åƒ")
                
                with gr.Row():
                    with gr.Column():
                        viz_method = gr.Radio(["PCA", "t-SNE"], value="PCA", label="é™ç»´æ–¹æ³•")
                        viz_btn = gr.Button("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–", variant="primary")
                        
                        gr.Markdown("""
                        **é™ç»´æ–¹æ³•è¯´æ˜:**
                        - **PCA**: çº¿æ€§é™ç»´ï¼Œä¿æŒå…¨å±€ç»“æ„
                        - **t-SNE**: éçº¿æ€§é™ç»´ï¼Œä¿æŒå±€éƒ¨ç»“æ„
                        """)
                    
                    with gr.Column():
                        viz_output = gr.Image(label="ç‰¹å¾ç©ºé—´å¯è§†åŒ–")
                        viz_text = gr.Textbox(label="è§£é‡Šè¯´æ˜", lines=12)
                
                viz_btn.click(
                    demo_instance.visualize_feature_space,
                    inputs=[viz_method],
                    outputs=[viz_output, viz_text]
                )
            
            # Tab 3: å…³äºDINOv2
            with gr.TabItem("ğŸ“š å…³äºDINOv2"):
                gr.Markdown("""
                ## ğŸ¤– ä»€ä¹ˆæ˜¯DINOv2?
                
                **DINOv2 (Self-Supervised Learning of Visual Features via Embedding Distillation)** æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è§†è§‰è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚
                
                ### âœ¨ ä¸»è¦ç‰¹ç‚¹:
                - **æ— éœ€æ ‡æ³¨**: ä¸éœ€è¦äººå·¥æ ‡æ³¨çš„æ•°æ®é›†
                - **å¼ºå¤§è¡¨ç¤º**: å­¦ä¹ åˆ°ä¸°å¯Œçš„è§†è§‰è¯­ä¹‰è¡¨ç¤º
                - **å¹¿æ³›é€‚ç”¨**: å¯ç”¨äºåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰å¤šç§ä»»åŠ¡
                - **å¼€ç®±å³ç”¨**: é¢„è®­ç»ƒæ¨¡å‹å¯ç›´æ¥ç”¨äºç‰¹å¾æå–
                
                ### ğŸ”¬ æŠ€æœ¯åŸç†:
                1. **è‡ªè’¸é¦å­¦ä¹ **: å­¦ç”Ÿç½‘ç»œå­¦ä¹ æ•™å¸ˆç½‘ç»œçš„è¡¨ç¤º
                2. **å¤šå°ºåº¦è®­ç»ƒ**: ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒå¢å¼ºæ³›åŒ–èƒ½åŠ›
                3. **Vision Transformer**: åŸºäºTransformeræ¶æ„çš„è§†è§‰æ¨¡å‹
                
                ### ğŸ¯ åº”ç”¨åœºæ™¯:
                - å›¾åƒæ£€ç´¢å’Œç›¸ä¼¼æ€§æœç´¢
                - é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
                - ç‰¹å¾æå–å’Œè¡¨ç¤ºå­¦ä¹ 
                - ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹
                
                ### ğŸ“– è®ºæ–‡ä¿¡æ¯:
                - **æ ‡é¢˜**: "DINOv2: Learning Robust Visual Features without Supervision"
                - **ä½œè€…**: Oquab et al. (Meta AI)
                - **å‘è¡¨**: 2023
                """)
        
        gr.Markdown("""
        ---
        ğŸ’¡ **ä½¿ç”¨æç¤º**: 
        - å°è¯•ä¸Šä¼ ä¸åŒç±»å‹çš„å›¾ç‰‡çœ‹çœ‹ç›¸ä¼¼æ€§æ•ˆæœ
        - è§‚å¯Ÿç‰¹å¾å¯è§†åŒ–ä¸­ç›¸ä¼¼å›¾ç‰‡çš„èšé›†æƒ…å†µ
        - æ€è€ƒï¼šä¸ºä»€ä¹ˆDINOv2èƒ½åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹å­¦åˆ°è¯­ä¹‰ä¿¡æ¯ï¼Ÿ
        """)
    
    return demo

if __name__ == "__main__":
    demo = create_demo()
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£
        share=False,            # ä¸ç”Ÿæˆpublicé“¾æ¥
        debug=True              # è°ƒè¯•æ¨¡å¼
    )